<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Derivation of Linear Regression Coefficients</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
        }
        h1 {
            color: #2c3e50;
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
        h2 {
            color: #3498db;
            margin-top: 30px;
            padding-top: 10px;
            border-top: 1px solid #eee;
        }
        h3 {
            color: #2980b9;
        }
        .math-block {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            overflow-x: auto;
        }
        .example {
            background-color: #f0f7fb;
            border-left: 5px solid #3498db;
            padding: 15px;
            margin: 20px 0;
        }
        .note {
            background-color: #fffacd;
            border-left: 5px solid #f1c40f;
            padding: 15px;
            margin: 20px 0;
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Mathematical Derivation of Linear Regression Coefficients</h1>

    <p>Linear regression models attempt to establish a linear relationship between a dependent variable \(y\) and an independent variable \(x\) that best represents the given dataset. This relationship is expressed as:</p>

    <div class="math-block">
        \[ y = \beta_0 + \beta_1 x \]
    </div>

    <p>Here, \(\beta_0\) represents the intercept (where the line crosses the y-axis) and \(\beta_1\) represents the slope. To find these parameters, we use the Ordinary Least Squares method.</p>

    <h2>1. The Foundation of Least Squares Method: Error Function</h2>

    <p>This method aims to minimize the sum of squared differences between the actual \(y\) values and the model's predictions. This sum is called the error function or cost function:</p>

    <div class="math-block">
        \[ J(\beta_0, \beta_1) = \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 x_i))^2 \]
    </div>

    <p>Where:</p>
    <ul>
        <li>\(y_i\) : Actual \(y\) values</li>
        <li>\(x_i\) : Independent variable values</li>
        <li>\(n\) : Number of data points</li>
    </ul>

    <h2>2. Finding the Minimum Point</h2>

    <p>To minimize the error function, we take partial derivatives with respect to \(\beta_0\) and \(\beta_1\) and set them equal to zero:</p>

    <h3>Partial derivative with respect to β₀:</h3>
    <div class="math-block">
        \[ \frac{\partial J}{\partial \beta_0} = -2 \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) \]
    </div>

    <h3>Partial derivative with respect to β₁:</h3>
    <div class="math-block">
        \[ \frac{\partial J}{\partial \beta_1} = -2 \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) \]
    </div>

    <h2>3. Setting Derivatives to Zero</h2>

    <p>At the minimum point, these derivatives must be zero.</p>

    <h3>Equation for β₀:</h3>
    <div class="math-block">
        \[ \frac{\partial J}{\partial \beta_0} = 0 \Rightarrow \sum_{i=1}^{n} (y_i - \beta_0 - \beta_1 x_i) = 0 \]
    </div>

    <p>Let's rearrange this equation:</p>

    <div class="math-block">
        \[ \sum_{i=1}^{n} y_i - n\beta_0 - \beta_1 \sum_{i=1}^{n} x_i = 0 \]
        \[ n\beta_0 = \sum_{i=1}^{n} y_i - \beta_1 \sum_{i=1}^{n} x_i \]
        \[ \beta_0 = \frac{\sum_{i=1}^{n} y_i}{n} - \beta_1 \frac{\sum_{i=1}^{n} x_i}{n} \]
    </div>

    <p>Using mean values:</p>
    <div class="math-block">
        \[ \beta_0 = \bar{y} - \beta_1 \bar{x} \]
    </div>

    <h3>Equation for β₁:</h3>
    <div class="math-block">
        \[ \frac{\partial J}{\partial \beta_1} = 0 \Rightarrow \sum_{i=1}^{n} x_i(y_i - \beta_0 - \beta_1 x_i) = 0 \]
    </div>

    <p>Expanding this equation:</p>
    <div class="math-block">
        \[ \sum_{i=1}^{n} x_i y_i - \beta_0 \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \]
    </div>

    <h2>4. Solving the System of Equations</h2>

    <p>Let's substitute the formula we found for \(\beta_0\) into the equation for \(\beta_1\):</p>

    <div class="math-block">
        \[ \sum_{i=1}^{n} x_i y_i - (\bar{y} - \beta_1 \bar{x}) \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \]
    </div>

    <p>Let's continue rearranging:</p>

    <div class="math-block">
        \[ \sum_{i=1}^{n} x_i y_i - \bar{y} \sum_{i=1}^{n} x_i + \beta_1 \bar{x} \sum_{i=1}^{n} x_i - \beta_1 \sum_{i=1}^{n} x_i^2 = 0 \]
        \[ \sum_{i=1}^{n} x_i y_i - \bar{y} \sum_{i=1}^{n} x_i = \beta_1 \sum_{i=1}^{n} x_i^2 - \beta_1 \bar{x} \sum_{i=1}^{n} x_i \]
    </div>

    <p>Using the fact that \(\sum_{i=1}^{n} x_i = n\bar{x}\):</p>

    <div class="math-block">
        \[ \sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y} = \beta_1 \sum_{i=1}^{n} x_i^2 - \beta_1 n \bar{x}^2 \]
    </div>

    <p>Now we can solve for \(\beta_1\):</p>

    <div class="math-block">
        \[ \beta_1 = \frac{\sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y}}{\sum_{i=1}^{n} x_i^2 - n \bar{x}^2} \]
    </div>

    <h2>5. Deriving a Simpler Formulation</h2>

    <p>Let's transform this formula into a more understandable form. First, let's rearrange the numerator:</p>

    <div class="math-block">
        \begin{align}
        \sum_{i=1}^{n} x_i y_i - n\bar{x}\bar{y} &= \sum_{i=1}^{n} x_i y_i - \bar{y}\sum_{i=1}^{n} x_i \\
        &= \sum_{i=1}^{n} (x_i y_i - \bar{y}x_i) \\
        &= \sum_{i=1}^{n} x_i(y_i - \bar{y})
        \end{align}
    </div>

    <p>Similarly for the denominator:</p>

    <div class="math-block">
        \begin{align}
        \sum_{i=1}^{n} x_i^2 - n \bar{x}^2 &= \sum_{i=1}^{n} x_i^2 - \bar{x}\sum_{i=1}^{n} x_i \\
        &= \sum_{i=1}^{n} (x_i^2 - \bar{x}x_i) \\
        &= \sum_{i=1}^{n} x_i(x_i - \bar{x})
        \end{align}
    </div>

    <p>Let's take one more step and note that:</p>

    <div class="math-block">
        \begin{align}
        \sum_{i=1}^{n} x_i(y_i - \bar{y}) &= \sum_{i=1}^{n} (x_i - \bar{x} + \bar{x})(y_i - \bar{y}) \\
        &= \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) + \bar{x}\sum_{i=1}^{n}(y_i - \bar{y})
        \end{align}
    </div>

    <p>The last term is zero because \(\sum_{i=1}^{n}(y_i - \bar{y}) = 0\). Similarly, the denominator expression can be simplified. As a result:</p>

    <div class="math-block">
        \[ \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{Cov(x,y)}{Var(x)} \]
    </div>

    <div class="note">
        <p>Here, \(Cov(x,y)\) represents the covariance between x and y, and \(Var(x)\) represents the variance of x.</p>
    </div>

    <h2>6. Final Formulas</h2>

    <p>Thus, with the ordinary least squares method, we obtain the following formulas for linear regression coefficients:</p>

    <h3>Slope (β₁):</h3>
    <div class="math-block">
        \[ \beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \frac{Cov(x,y)}{Var(x)} \]
    </div>

    <h3>Intercept (β₀):</h3>
    <div class="math-block">
        \[ \beta_0 = \bar{y} - \beta_1 \bar{x} \]
    </div>

    <h2>7. Step-by-Step Calculation with an Example</h2>

    <div class="example">
        <p>Let's perform a step-by-step calculation with a small dataset:</p>
        <ul>
            <li>x = [1, 2, 3, 4, 5]</li>
            <li>y = [2, 3.5, 5, 6.2, 8]</li>
        </ul>

        <h3>Step 1: Calculate the means</h3>
        <p>\(\bar{x} = \frac{1+2+3+4+5}{5} = 3\)</p>
        <p>\(\bar{y} = \frac{2+3.5+5+6.2+8}{5} = 4.94\)</p>

        <h3>Step 2: Calculate β₁</h3>
        <p>First, the numerator:</p>
        <p>\((1-3)(2-4.94) + (2-3)(3.5-4.94) + (3-3)(5-4.94) + (4-3)(6.2-4.94) + (5-3)(8-4.94)\)</p>
        <p>\(= (-2)(-2.94) + (-1)(-1.44) + (0)(0.06) + (1)(1.26) + (2)(3.06)\)</p>
        <p>\(= 5.88 + 1.44 + 0 + 1.26 + 6.12 = 14.7\)</p>

        <p>Then, the denominator:</p>
        <p>\((1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2\)</p>
        <p>\(= 4 + 1 + 0 + 1 + 4 = 10\)</p>

        <p>Therefore:</p>
        <p>\(\beta_1 = \frac{14.7}{10} = 1.47\)</p>

        <h3>Step 3: Calculate β₀</h3>
        <p>\(\beta_0 = 4.94 - 1.47 \times 3 = 4.94 - 4.41 = 0.53\)</p>

        <h3>Step 4: Write the regression equation</h3>
        <p>\(y = 0.53 + 1.47x\)</p>

        <p>This is the best-fitting linear model for our data. For each unit increase in x, y will increase by approximately 1.47 units, and when x=0, y will be 0.53.</p>
    </div>

    <h2>Conclusion</h2>

    <p>The Ordinary Least Squares method is a mathematical technique used to find the parameters of linear regression. This method attempts to minimize the sum of squared errors and results in a line that minimizes the difference between our predictions and the actual values. The derivation steps above clearly show where these formulas come from.</p>

    <p>This method is one of the fundamental building blocks of statistical modeling and data analysis, and forms the foundation of many modern machine learning algorithms.</p>
</body>
</html>