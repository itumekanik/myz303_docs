<!DOCTYPE html>
<html lang="tr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradyan İniş Teorisi - MYZ 303E</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <style>
        :root {
            --primary-color: #2c3e50;
            --secondary-color: #3498db;
            --accent-color: #e74c3c;
            --light-bg: #ecf0f1;
            --dark-text: #2c3e50;
            --light-text: #ecf0f1;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: var(--dark-text);
            background-color: var(--light-bg);
            margin: 0;
            padding: 0;
        }
        
        header {
            background-color: var(--primary-color);
            color: var(--light-text);
            text-align: center;
            padding: 2rem 1rem;
            margin-bottom: 2rem;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 0 2rem 4rem;
            background-color: white;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        
        h1 {
            margin: 0;
            font-size: 2.5rem;
        }
        
        h2 {
            color: var(--primary-color);
            border-bottom: 2px solid var(--secondary-color);
            padding-bottom: 0.5rem;
            margin-top: 2.5rem;
        }
        
        h3 {
            color: var(--primary-color);
            margin-top: 1.8rem;
        }
        
        p {
            margin-bottom: 1.2rem;
        }
        
        .math-block {
            background-color: #f8f9fa;
            padding: 1rem;
            border-left: 4px solid var(--secondary-color);
            font-family: "Cambria Math", Georgia, serif;
            margin: 1.5rem 0;
            text-align: center;
        }
        
        .note {
            background-color: #fef9e7;
            border-left: 4px solid #f39c12;
            padding: 1rem;
            margin: 1.5rem 0;
        }
        
        .note-title {
            font-weight: bold;
            margin-bottom: 0.5rem;
            color: #d35400;
        }
        
        ol, ul {
            padding-left: 2rem;
        }
        
        li {
            margin-bottom: 0.8rem;
        }
        
        footer {
            background-color: var(--primary-color);
            color: var(--light-text);
            text-align: center;
            padding: 1rem;
            margin-top: 2rem;
        }
        
        .navigation {
            display: flex;
            justify-content: space-between;
            margin-top: 3rem;
            padding-top: 1.5rem;
            border-top: 1px solid #ddd;
        }
        
        .nav-button {
            display: inline-block;
            background-color: var(--secondary-color);
            color: white;
            padding: 0.7rem 1.2rem;
            border-radius: 4px;
            text-decoration: none;
            font-weight: bold;
            transition: background-color 0.3s ease;
        }
        
        .nav-button:hover {
            background-color: var(--primary-color);
        }
        
        .back-to-index {
            background-color: #7f8c8d;
        }
    </style>
</head>
<body>
    <header>
        <h1>Gradyan İniş Teorisi</h1>
        <p>MYZ 303E İnşaat Mühendisliğinde Yapay Zeka</p>
    </header>
    
    <div class="container">
        <h2>Gradyan İnişe Giriş</h2>
        <p>Gradyan İniş, makine öğrenmesinde bir fonksiyonun minimumunu bulmak için kullanılan temel bir optimizasyon algoritmasıdır. Makine öğrenmesi bağlamında, bu fonksiyon genellikle bir modelin belirli bir veri setinde ne kadar iyi performans gösterdiğini ölçen bir kayıp veya maliyet fonksiyonudur. Amaç, en iyi modeli elde etmek için bu fonksiyonu minimize eden ağırlık değerlerini (W) bulmaktır.</p>
        
        <p>Özünde, gradyan iniş, en dik yokuş aşağı yönünde adımlar atarak bir vadideki en düşük noktayı bulmaya benzer. Algoritma, adını fonksiyonun minimumuna ulaşmak için negatif gradyanı (en dik iniş yönü) takip etmesinden alır.</p>
        
        <h2>Matematiksel Temeller</h2>
        
        <h3>Gradyan</h3>
        <p>Bir fonksiyonun gradyanı, kısmi türevlerin bir vektörüdür. f(x₁, x₂, ..., xₙ) fonksiyonu için gradyan ∇f olarak gösterilir:</p>
        
        <div class="math-block">
            \[ \nabla f = \left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right] \]
        </div>
        
        <p>Gradyan, fonksiyonun en dik artış yönünü gösterir. Ters yönde (-∇f) adımlar atarak, fonksiyonun minimumuna doğru ilerleyebiliriz. Makine öğrenmesinde, gradyanı modelimizin ağırlıklarına (W) göre hesaplarız.</p>
        
        <h3>Güncelleme Kuralı</h3>
        <p>Gradyan inişte, ağırlıkları (W) aşağıdaki kurala göre iteratif olarak güncelleriz:</p>
        
        <div class="math-block">
            \[ W = W - \alpha \nabla J(W) \]
        </div>
        
        <p>Burada:</p>
        <ul>
            <li>W, modelin ağırlıklarını temsil eder</li>
            <li>α (alfa), her adımın büyüklüğünü kontrol eden öğrenme oranıdır</li>
            <li>∇J(W), maliyet fonksiyonu J'nin ağırlıklara (W) göre gradyanıdır</li>
        </ul>
        
        <div class="note">
            <div class="note-title">Önemli Not:</div>
            <p>Öğrenme oranı α kritik bir hiperparametredir. Çok küçükse, algoritma çok yavaş yakınsayacaktır. Çok büyükse, algoritma minimumu aşabilir ve yakınsamayı başaramayabilir veya hatta uzaklaşabilir.</p>
        </div>
        
        <h2>Gradyan İniş Türleri</h2>
        
        <h3>Toplu Gradyan İniş (Batch Gradient Descent)</h3>
        <p>Toplu gradyan inişte, tüm eğitim veri seti için maliyet fonksiyonunun ağırlıklara göre gradyanını hesaplarız:</p>
        
        <div class="math-block">
            \[ W = W - \alpha \nabla J(W) \]
        </div>
        
        <p>Bu yaklaşım, büyük veri setleri için hesaplama açısından pahalıdır çünkü her güncelleme adımı için tüm eğitim seti üzerinde gradyanların hesaplanmasını gerektirir.</p>
        
        <h3>Stokastik Gradyan İniş (SGD)</h3>
        <p>Stokastik gradyan iniş, parametreleri bir seferde yalnızca bir eğitim örneği kullanarak günceller:</p>
        
        <div class="math-block">
            \[ W = W - \alpha \nabla J(W; x^{(i)}, y^{(i)}) \]
        </div>
        
        <p>Burada (x⁽ⁱ⁾, y⁽ⁱ⁾) tek bir eğitim örneğidir. SGD çok daha hızlıdır, ancak parametre güncellemelerinde daha yüksek varyansa sahiptir, bu da amaç fonksiyonunun şiddetli dalgalanmasına neden olabilir.</p>
        
        <h3>Mini-Batch Gradyan İniş</h3>
        <p>Mini-batch gradyan iniş, toplu ve stokastik yöntemler arasında bir uzlaşmadır. Ağırlıkları, eğitim verisinin küçük bir rastgele alt kümesini (mini-batch) kullanarak günceller:</p>
        
        <div class="math-block">
            \[ W = W - \alpha \nabla J(W; x^{(i:i+n)}, y^{(i:i+n)}) \]
        </div>
        
        <p>Bu yaklaşım, SGD'ye kıyasla parametre güncellemelerinin varyansını azaltarak daha stabil bir yakınsama sağlar. Ayrıca, paralelleştirilebilen verimli matris işlemlerine olanak tanır.</p>
        
        <h2>Yakınsama ve Zorluklar</h2>
        
        <h3>Yakınsama Kriterleri</h3>
        <p>Gradyan iniş genellikle şu koşullardan biri karşılandığında sonlandırılır:</p>
        <ul>
            <li>Maliyet fonksiyonu J(W)'deki değişim önceden tanımlanmış bir eşiğin altına düştüğünde</li>
            <li>Gradyanın büyüklüğü ∇J(W) bir eşiğin altına düştüğünde</li>
            <li>Maksimum iterasyon sayısına ulaşıldığında</li>
        </ul>
        
        <h3>Gradyan İnişte Zorluklar</h3>
        
        <h4>Yerel Minimumlar</h4>
        <p>Konveks olmayan fonksiyonlar için (derin öğrenmedekiler gibi), gradyan iniş global minimum yerine bir yerel minimuma yakınsayabilir. Bunun nedeni, algoritmanın sadece yokuş aşağı hareket etmesi ve küresel olarak en derin nokta olmayan vadilere takılabilmesidir.</p>
        
        <h4>Eyer Noktaları</h4>
        <p>Eyer noktaları, gradyanın tüm yönlerde sıfır olduğu, ancak minimum olmayan noktalardır. Yüksek boyutlu uzaylarda (makine öğrenmesinde yaygın), eyer noktaları yerel minimumlardan daha yaygındır ve yakınsama hızını düşürebilir.</p>
        
        <h4>Platolar</h4>
        <p>Platolar, gradyanın çok küçük olduğu ancak sıfır olmadığı düz bölgelerdir. Gradyan iniş bu bölgelerde önemli ölçüde yavaşlayabilir ve kaçmak için birçok iterasyon gerekebilir.</p>
        
        <h2>Gelişmiş Gradyan İniş Varyantları</h2>
        
        <h3>Momentum</h3>
        <p>Momentum, mevcut güncellemeye önceki güncelleme vektörünün bir fraksiyonunu ekleyerek gradyan inişi hızlandırmaya yardımcı olur:</p>
        
        <div class="math-block">
            \[ v = \gamma v - \alpha \nabla J(W) \]
            \[ W = W + v \]
        </div>
        
        <p>Burada γ momentum katsayısıdır (genellikle 0.9). Bu, algoritmanın ravines (yüzeyin bir boyutta diğerine göre çok daha dik eğrildiği alanlar) içinde daha etkili bir şekilde hareket etmesine yardımcı olur.</p>
        
        <h3>RMSprop</h3>
        <p>RMSprop, kare gradyanların geçmişine dayalı olarak her ağırlık için öğrenme oranını ayarlar:</p>
        
        <div class="math-block">
            \[ E[g^2]_t = 0.9E[g^2]_{t-1} + 0.1(\nabla J(W))^2 \]
            \[ W = W - \frac{\alpha}{\sqrt{E[g^2]_t + \epsilon}}\nabla J(W) \]
        </div>
        
        <p>Bu, gradyanı normalleştirmeye yardımcı olur ve algoritmayı özelliklerin ölçeğine daha az duyarlı hale getirir.</p>
        
        <h3>Adam (Adaptif Moment Tahmini)</h3>
        <p>Adam, momentum ve RMSprop fikirlerini birleştirir, gradyanların hem ilk momentini (ortalama) hem de ikinci momentini (merkezsiz varyans) takip eder:</p>
        
        <div class="math-block">
            \[ m = \beta_1 m + (1-\beta_1)\nabla J(W) \]
            \[ v = \beta_2 v + (1-\beta_2)(\nabla J(W))^2 \]
            \[ \hat{m} = \frac{m}{1-\beta_1^t} \]
            \[ \hat{v} = \frac{v}{1-\beta_2^t} \]
            \[ W = W - \frac{\alpha}{\sqrt{\hat{v}} + \epsilon}\hat{m} \]
        </div>
        
        <p>Adam, genellikle temel gradyan inişten daha hızlı ve daha güvenilir bir şekilde yakınsadığı için uygulamada yaygın olarak kullanılır.</p>
        
        <h2>İnşaat Mühendisliğinde Uygulamalar</h2>
        
        <p>Gradyan iniş, makine öğrenmesinin inşaat mühendisliğindeki uygulamalarında yaygın olarak kullanılır, bunlara şunlar dahildir:</p>
        
        <ul>
            <li><strong>Yapısal Optimizasyon:</strong> Güvenlik kısıtlamalarını korurken ağırlığı minimize eden optimal tasarım parametrelerini bulmak.</li>
            <li><strong>Malzeme Modellemesi:</strong> Deneysel verilere uygun bünye modellerinin parametrelerini kalibre etmek.</li>
            <li><strong>Trafik Akışı Tahmini:</strong> Trafik modellerini tahmin etmek ve ulaşım sistemlerini optimize etmek için sinir ağlarını eğitmek.</li>
            <li><strong>Yapısal Sağlık İzleme:</strong> Sensör verilerinden yapısal hasarı tespit edebilen ve sınıflandırabilen modeller geliştirmek.</li>
            <li><strong>İnşaat Proje Yönetimi:</strong> Kaynak tahsisi ve proje planlamasını optimize etmek.</li>
        </ul>
        
        <div class="note">
            <div class="note-title">İnşaat Mühendisliği Perspektifi:</div>
            <p>İnşaat mühendisliği uygulamalarında, kayıp fonksiyonunun seçimi özellikle önemlidir. Yapısal uygulamalar için, güvenlikle ilgili kaygılar genellikle düşük tahminleri aşırı tahminlerden daha ciddi şekilde cezalandıran asimetrik kayıp fonksiyonlarını gerektirir.</p>
        </div>
        
        <h2>Sonuç</h2>
        
        <p>Gradyan iniş, çoğu makine öğrenmesi algoritmasının omurgasını oluşturan güçlü bir optimizasyon tekniğidir. Matematiksel temellerini, varyantlarını ve sınırlamalarını anlamak, makine öğrenmesini inşaat mühendisliği problemlerine etkili bir şekilde uygulamak için gereklidir.</p>
        
        <p>Bu öğretici teorik yönlere odaklanırken, pratik uygulama, hiperparametrelerin, verilerin ön işlenmesinin ve belirli problem için uygun model mimarilerinin seçiminin dikkatli bir şekilde düşünülmesini gerektirir.</p>
        
        <div class="navigation">
            <a href="index.html" class="nav-button back-to-index">Ders İndeksine Dön</a>
            <a href="gradient_descent_with_single_W.html" class="nav-button">İnteraktif Gradyan İniş Aracını Dene</a>
        </div>
    </div>
    
    <footer>
        <p>&copy; 2025 MYZ 303E İnşaat Mühendisliğinde Yapay Zeka</p>
    </footer>
</body>
</html>